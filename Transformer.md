# Transformer

![image-20241029170241984](images\image-20241029170241984.png)

![image-20241029170553015](images/image-20241029170553015.png)

v,k,q通过h个线性投影之后输入到h个缩小点积注意力

decoder 堆栈中的自注意力子层进行了修改，以防止位置关注后续位置。这种掩蔽，加上输出嵌入偏移一个位置的事实，确保了位置i*i*的预测只能依赖于位置小于i*i*的已知输出。